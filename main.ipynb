{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "codelab link: https://colab.research.google.com/github/abzzall/unsupervised_annotator1/blob/main/main.ipynb\n",
    "github link: https://github.com/abzzall/unsupervised_annotator1.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46554966da38c2ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Extract multiword expression candidates. \n",
    "\t1. Using the part-of-speech tags we extract multiword expression candidates, consisting of sequences of zero or more adjectives (ADJ followed by nouns (NOUN) or proper nouns (PROPNs) sequences.\n",
    "\t2. To generate training data for sequence tagging use sentence encoder like \n",
    "\t\ta. EmbedRank (Bennani- Smires et al., 2018\n",
    "\t\tb. Key2Vec (Mahata et al., 2018)\n",
    "    3. We implemented our Unsupervised Annotator using the POS tagger of SpaCy (Honnibal et al., 2020)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa6f429b03250b9"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['Requirement already satisfied: spacy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (3.6.1)',\n 'Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (3.0.12)',\n 'Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (1.0.4)',\n 'Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (1.0.9)',\n 'Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (2.0.7)',\n 'Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (3.0.8)',\n 'Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (8.1.12)',\n 'Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (1.1.2)',\n 'Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (2.4.7)',\n 'Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (2.0.9)',\n 'Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (0.9.0)',\n 'Requirement already satisfied: pathy>=0.10.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (0.10.2)',\n 'Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (6.3.0)',\n 'Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (4.66.1)',\n 'Requirement already satisfied: numpy>=1.15.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (1.25.2)',\n 'Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (2.31.0)',\n 'Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (2.3.0)',\n 'Requirement already satisfied: jinja2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (3.1.2)',\n 'Requirement already satisfied: setuptools in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (68.0.0)',\n 'Requirement already satisfied: packaging>=20.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (23.1)',\n 'Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from spacy) (3.3.0)',\n 'Requirement already satisfied: annotated-types>=0.4.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)',\n 'Requirement already satisfied: pydantic-core==2.6.3 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)',\n 'Requirement already satisfied: typing-extensions>=4.6.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)',\n 'Requirement already satisfied: charset-normalizer<4,>=2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)',\n 'Requirement already satisfied: idna<4,>=2.5 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)',\n 'Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)',\n 'Requirement already satisfied: certifi>=2017.4.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)',\n 'Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)',\n 'Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)',\n 'Requirement already satisfied: colorama in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)',\n 'Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)',\n 'Requirement already satisfied: MarkupSafe>=2.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from jinja2->spacy) (2.1.3)']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "!! pip install spacy "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:07:41.427550600Z",
     "start_time": "2023-10-03T17:07:38.305186800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['Requirement already satisfied: sentence-transformers in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (2.2.2)',\n 'Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (4.33.0)',\n 'Requirement already satisfied: tqdm in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (4.66.1)',\n 'Requirement already satisfied: torch>=1.6.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (2.0.1)',\n 'Requirement already satisfied: torchvision in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.15.2)',\n 'Requirement already satisfied: numpy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.25.2)',\n 'Requirement already satisfied: scikit-learn in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.3.0)',\n 'Requirement already satisfied: scipy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.11.2)',\n 'Requirement already satisfied: nltk in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (3.8.1)',\n 'Requirement already satisfied: sentencepiece in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.1.99)',\n 'Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.16.4)',\n 'Requirement already satisfied: filelock in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.3)',\n 'Requirement already satisfied: fsspec in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.0)',\n 'Requirement already satisfied: requests in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)',\n 'Requirement already satisfied: pyyaml>=5.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)',\n 'Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)',\n 'Requirement already satisfied: packaging>=20.9 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)',\n 'Requirement already satisfied: sympy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)',\n 'Requirement already satisfied: networkx in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)',\n 'Requirement already satisfied: jinja2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)',\n 'Requirement already satisfied: colorama in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from tqdm->sentence-transformers) (0.4.6)',\n 'Requirement already satisfied: regex!=2019.12.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)',\n 'Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)',\n 'Requirement already satisfied: safetensors>=0.3.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)',\n 'Requirement already satisfied: click in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from nltk->sentence-transformers) (8.1.7)',\n 'Requirement already satisfied: joblib in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from nltk->sentence-transformers) (1.3.2)',\n 'Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)',\n 'Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torchvision->sentence-transformers) (10.0.0)',\n 'Requirement already satisfied: MarkupSafe>=2.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)',\n 'Requirement already satisfied: charset-normalizer<4,>=2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)',\n 'Requirement already satisfied: idna<4,>=2.5 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)',\n 'Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)',\n 'Requirement already satisfied: certifi>=2017.4.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)',\n 'Requirement already satisfied: mpmath>=0.19 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!! python -m spacy download en_core_web_sm\n",
    "!!pip install sentence-transformers\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:07:58.970988600Z",
     "start_time": "2023-10-03T17:07:41.427550600Z"
    }
   },
   "id": "cc3d3003d03d7b9e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:08:08.646709900Z",
     "start_time": "2023-10-03T17:07:58.973988700Z"
    }
   },
   "id": "fb5b4593d7f8076e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# Define your text\n",
    "with open('abstracts.txt') as file:\n",
    "    text=file.read()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:08:08.657258700Z",
     "start_time": "2023-10-03T17:08:08.647715200Z"
    }
   },
   "id": "c77006b00543dedb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# text = \"I love New York and San Francisco. Los Angeles is another great city.\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:08:08.659259400Z",
     "start_time": "2023-10-03T17:08:08.655249500Z"
    }
   },
   "id": "35f06edc031c9adb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "CandidateMWE = namedtuple('CandidateMWE',['text','head', 'sentence','self_encode', 'sent_encode'])\n",
    "CandidateW=namedtuple('CandidateW',['text','lemma', 'self_encode' ])\n",
    "Term=namedtuple('Term',['text','detected' ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T17:08:08.666462500Z",
     "start_time": "2023-10-03T17:08:08.660258500Z"
    }
   },
   "id": "75546a0cd3d2318d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n",
      "context aware expectation maximization is not candidate 2 aware -- ADJ, True\n",
      "label-rich source data is not candidate 2 rich -- ADJ, True\n",
      "image-level weak labels is not candidate 2 weak -- ADJ, True\n",
      "category-wise domain alignment is not candidate 2 wise -- ADJ, True\n"
     ]
    }
   ],
   "source": [
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract MWEs (noun phrases) from the text\n",
    "mwe_list = []\n",
    "single_noun_list=[]\n",
    "candidate_list=[]\n",
    "for sent in doc.sents:\n",
    "    sent_encode=model.encode(sent.text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        is_candidate=False\n",
    "        word_count=0\n",
    "        # mwe_list.append(chunk.text)\n",
    "        if len(chunk.text.split()) > 1:\n",
    "            noun_appeared=False\n",
    "            is_candidate=True\n",
    "            cleared_candidate=''\n",
    "            for word in chunk:\n",
    "                #IGNORING\n",
    "                if word.pos_ in ['PUNCT', 'DET']:\n",
    "                    continue\n",
    "                elif word.pos_ not in ['ADJ', 'PROPN', 'NOUN']:\n",
    "                    is_candidate=False\n",
    "                    # print(f'{chunk.text} is not candidate 1 {word.text} -- {word.pos_}')\n",
    "                    break\n",
    "                elif word.pos_ in ['PROPN', 'NOUN']:\n",
    "                    noun_appeared=True\n",
    "                elif not(not noun_appeared and word.pos_=='ADJ'):\n",
    "                    is_candidate=False\n",
    "                    print(f'{chunk.text} is not candidate 2 {word.text} -- {word.pos_}, {noun_appeared}')\n",
    "                    break\n",
    "                cleared_candidate+=' '+word.text\n",
    "                word_count+=1\n",
    "        if is_candidate and word_count>1:\n",
    "            cleared_candidate.strip()\n",
    "            candidate_list.append(CandidateMWE(cleared_candidate, chunk.root.text, sent.text, model.encode(cleared_candidate), sent_encode))\n",
    "            # print(cleared_candidate)\n",
    "        else:\n",
    "            single_noun_list +=[CandidateW(word.text, word.lemma_, model.encode(word.text)) for word in chunk if word.pos_ in ['NOUN', 'PROPN']]\n",
    "                    \n",
    "mwe_list=candidate_list+single_noun_list\n",
    "# print(mwe_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-03T17:08:08.664465700Z"
    }
   },
   "id": "1e716c1702eba543"
  },
  {
   "cell_type": "raw",
   "source": [
    "2. Filter candidates by specificity or topic score.\n",
    "\t1. Topic score. The topic score captures the similarity, topic-wise, between a candidate and the sentence containing it. It is computed as the cosine similarity between the embedding vector of the multiword expression and the embedding vector of the sentence containing it. \n",
    "\t2. Specificity score (SP). This is the mean of the pairwise distance, in the embedding space, between the multiword expressions and all the other word or multiword expression in the context. Specifically, given a multiword mw, and the word or multiword expression w1; :::;wk in its context, we define the specificity score SP as: \n",
    "\t\n",
    "\t\ta. SP(mw)=sum(dist(wi, mw))/k\n",
    "\t\twhere dist(wi;wj) is the cosine-similarity between the embedding vectors of wi and wj . Multiword expressions with a higher score correspond to more specific terms.\n",
    "\t3. In our implementation we use the pretrained sentence encoders described in Reimers and Gurevych (2019),\n",
    "\t\ta. To compute the specificity and similarity scores we use the sentence embedding model distilbert-base-nli-mean-tokens from the sentence transformers library. pypi.org/project/sentence-transformers/. \n",
    "\t\t1. other sentence encoders can be used as a drop-in replacement.\n",
    "\t4. Multiword expressions with a specificity or topic score below a certain threshold can be filtered out.\n",
    "\t\ta. TSP = 0.05\n",
    "\t\tb. Ttopic = 0.1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34484dd181b5010f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dist(wi_encode, wj_encode)->float:\n",
    "    return util.pytorch_cos_sim(\n",
    "        wi_encode,\n",
    "        wj_encode\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_topic_score(expression_embedding, sentence_embedding)->float:\n",
    "    \"\"\"\n",
    "    Calculate the topic score between a multiword expression and a sentence.\n",
    "\n",
    "    Args:\n",
    "        multiword_expression (str): The multiword expression.\n",
    "        sentence (str): The sentence containing the expression.\n",
    "\n",
    "    Returns:\n",
    "        float: The topic score (cosine similarity) between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Load the distilbert-base-nli-mean-tokens model\n",
    "\n",
    "    # Encode the multiword expression and sentence into embeddings\n",
    "    # expression_embedding = model.encode(multiword_expression, convert_to_tensor=True)\n",
    "    # sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity between the two embeddings\n",
    "    similarity_score = util.pytorch_cos_sim(expression_embedding, sentence_embedding)\n",
    "\n",
    "    # Extract the cosine similarity value from the tensor\n",
    "    topic_score = similarity_score[0].item()\n",
    "\n",
    "    return topic_score\n",
    "\n",
    "\n",
    "def calculate_specificity_score(mw:CandidateMWE, w:List[CandidateW|CandidateMWE])->float:\n",
    "    \"\"\"\n",
    "    Calculate the specificity score (SP) between a multiword expression (mw) and a list of words/multiword expressions (w).\n",
    "\n",
    "    Args:\n",
    "        mw (str): The multiword expression.\n",
    "        w (list of str): The list of words/multiword expressions in the context.\n",
    "\n",
    "    Returns:\n",
    "        float: The specificity score (SP).\n",
    "    \"\"\"\n",
    "    # Load the distilbert-base-nli-mean-tokens model\n",
    "    # Calculate distances between mw and each word/phrase in w\n",
    "    distances = [dist(mw.self_encode, wi.self_encode) for wi in w if wi.text != mw.text]\n",
    "\n",
    "    # Calculate the mean of the distances\n",
    "    specificity_score = sum(distances) / len(w)\n",
    "\n",
    "    return specificity_score\n",
    "\n",
    "def calculate_specifity_score_context_itself(mw:CandidateMWE):\n",
    "    candidate_embedding=mw.self_encode\n",
    "    s=0\n",
    "    for word in mw.text.split():\n",
    "        word_embedding=model.encode(word, convert_to_tensor=True)\n",
    "        distance=dist(candidate_embedding, word_embedding)\n",
    "        s+=distance\n",
    "    return s/len(mw.text.split())\n",
    "\n",
    "def calculate_specifity_score_context_sentence(mw:CandidateMWE):\n",
    "    candidate_embedding=mw.self_encode\n",
    "    s=0\n",
    "    c=0\n",
    "    sentence=mw.sentence\n",
    "    doc=nlp(sentence)\n",
    "    for word in doc.noun_chunks:\n",
    "        word_embedding=model.encode(word, convert_to_tensor=True)\n",
    "        distance=dist(candidate_embedding, word_embedding)\n",
    "        s+=distance\n",
    "        c+=1\n",
    "    return s/len(c)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6203549fb3899ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TSP = 0.05\n",
    "Ttopic = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "257ab1172a63c0f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a2ffe0272f5ee03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First Variant"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b68ec541b3f3f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_mws1=[]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aabdd9151f59fd0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time=datetime.now()\n",
    "for candidate in candidate_list:\n",
    "    topic_score=calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "    sp_score=calculate_specificity_score(candidate, mwe_list)\n",
    "    if topic_score>Ttopic and sp_score>TSP:\n",
    "        term_mws1.append(Term(candidate.text, 'by_score'))\n",
    "        print(candidate.text, topic_score, sp_score)\n",
    "end_time=datetime.now()\n",
    "delta=end_time-start_time\n",
    "print(f'duration: {delta.seconds}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a7764eb22a80e680"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second Variant. Filtering out by topic score first"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ce21e96a7b54deb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_mws2=[]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "613ca9dcb48a0b3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time=datetime.now()\n",
    "\n",
    "temp_candidate=[]\n",
    "for candidate in candidate_list:\n",
    "    topic_score=calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "    # sp_score=calculate_specificity_score(candidate, mwe_list)\n",
    "    if topic_score>Ttopic:\n",
    "        temp_candidate.append(candidate)\n",
    "for candidate in temp_candidate:\n",
    "    # topic_score=calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "    sp_score=calculate_specificity_score(candidate, temp_candidate)\n",
    "    if sp_score>TSP:\n",
    "        term_mws2.append(Term(candidate.text, 'by_score'))        \n",
    "        print(candidate.text, sp_score)\n",
    "end_time=datetime.now()\n",
    "delta=end_time-start_time\n",
    "print(f'duration: {delta.seconds}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e52f8c5643dfff70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Third Variant: Context is itself"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc77db19f78c3f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_mws3=[]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e2af44a535582d94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time=datetime.now()\n",
    "\n",
    "for candidate in candidate_list:\n",
    "    topic_score=calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "    sp_score=calculate_specifity_score_context_itself(candidate)\n",
    "    if topic_score>Ttopic and sp_score>TSP:\n",
    "        term_mws3.append(Term(candidate.text, 'by_score'))\n",
    "        print(candidate.text, topic_score, sp_score)\n",
    "end_time=datetime.now()\n",
    "delta=end_time-start_time\n",
    "print(f'duration: {delta.seconds}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1eb4955481cf24bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Forth Variant: Context is sentence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60a896073838b1a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_mws3 = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a7651102a20fd745"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time=datetime.now()\n",
    "\n",
    "for candidate in candidate_list:\n",
    "\ttopic_score = calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "\tsp_score = calculate_specifity_score_context_sentence(candidate)\n",
    "\tif topic_score > Ttopic and sp_score > TSP:\n",
    "\t\tterm_mws3.append(Term(candidate.text, 'by_score'))\n",
    "\t\tprint(candidate.text, topic_score, sp_score)\n",
    "end_time=datetime.now()\n",
    "delta=end_time-start_time\n",
    "print(f'duration: {delta.seconds}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "87aedd9e66c1e1c2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb27c43445c67d52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Upgrade single nouns according to morphological features.\n",
    "\t1. At this stage, we could have nouns that are not part of any multiword expressions, but still relevant.\n",
    "\t2. Check if the lemma of the noun is the same as any of the heads of the multiword expressions.\n",
    "\t\ta. Yes: we upgrade the noun to term \n",
    "\t\tb. No: segment the word using a subword-unit segmentation and a vocabulary trained over a large general purpose corpus.\n",
    "we use the vocabulary of the BERT-base model from HuggingFace (Wolf et al., 2020) and the corresponding tokenizer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d5e0d6670d5f310"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8d83d9d8fb91f457"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f9ddd1da71dd8608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_nouns=[]\n",
    "subtoken_threshold=4\n",
    "for candidate in single_noun_list:\n",
    "    #Check if the lemma of the noun is the same as any of the heads of the multiword expressions.\n",
    "    is_term=False\n",
    "    lemma_is_head=False\n",
    "    for term_mw in term_mws:\n",
    "        if term_mw.head==candidate.lemma:\n",
    "            is_term=True\n",
    "            break\n",
    "    if is_term:\n",
    "        term_nouns.append(Term(candidate.text, 'by_lemma'))\n",
    "        continue\n",
    "    #segment the word using a subword-unit segmentation and a vocabulary trained over a large general purpose corpus.\n",
    "    subtokens=tokenizer.tokenize(candidate.text)\n",
    "    if len(subtokens)>subtoken_threshold:\n",
    "        term_nouns.append(Term(candidate.text, 'by_subtokens'))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6d91c0e70d37709"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "terms=term_mws+term_nouns\n",
    "with open('out.txt', 'w') as out_file:\n",
    "    for term in terms:\n",
    "        out_file.write(f'\"{term.text}\" appended {term.detected}\\n')\n",
    "        print(f'\"{term.text}\" appended {term.detected}\\n')\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5ef2038896ab2162"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "16f701bdf06e9728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2b6623bcdf9b97e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6981e177f3ad4f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
