{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Installing necessary modules and run necessary scripts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d84faeb83ad2746"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "['Looking in indexes: https://download.pytorch.org/whl/cu121',\n 'Requirement already satisfied: torch in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (2.1.0+cu121)',\n 'Requirement already satisfied: torchvision in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (0.16.0+cu121)',\n 'Requirement already satisfied: torchaudio in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (2.1.0+cu121)',\n 'Requirement already satisfied: filelock in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (3.12.3)',\n 'Requirement already satisfied: typing-extensions in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (4.7.1)',\n 'Requirement already satisfied: sympy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (1.12)',\n 'Requirement already satisfied: networkx in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (3.1)',\n 'Requirement already satisfied: jinja2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (3.1.2)',\n 'Requirement already satisfied: fsspec in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch) (2023.9.0)',\n 'Requirement already satisfied: numpy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torchvision) (1.25.2)',\n 'Requirement already satisfied: requests in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torchvision) (2.31.0)',\n 'Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torchvision) (10.0.0)',\n 'Requirement already satisfied: MarkupSafe>=2.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from jinja2->torch) (2.1.3)',\n 'Requirement already satisfied: charset-normalizer<4,>=2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->torchvision) (3.2.0)',\n 'Requirement already satisfied: idna<4,>=2.5 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->torchvision) (3.4)',\n 'Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->torchvision) (2.0.4)',\n 'Requirement already satisfied: certifi>=2017.4.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->torchvision) (2023.7.22)',\n 'Requirement already satisfied: mpmath>=0.19 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sympy->torch) (1.3.0)',\n '',\n '[notice] A new release of pip is available: 23.2.1 -> 23.3.1',\n '[notice] To update, run: python.exe -m pip install --upgrade pip']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "!!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:14.558382400Z",
     "start_time": "2023-10-30T08:59:11.836997500Z"
    }
   },
   "id": "fc1fc3aea952fb9c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:14.561382100Z",
     "start_time": "2023-10-30T08:59:14.425375700Z"
    }
   },
   "id": "752767abe5f7ecd0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['Requirement already satisfied: sentence-transformers in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (2.2.2)',\n 'Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (4.33.0)',\n 'Requirement already satisfied: tqdm in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (4.66.1)',\n 'Requirement already satisfied: torch>=1.6.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (2.1.0+cu121)',\n 'Requirement already satisfied: torchvision in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.16.0+cu121)',\n 'Requirement already satisfied: numpy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.25.2)',\n 'Requirement already satisfied: scikit-learn in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.3.0)',\n 'Requirement already satisfied: scipy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (1.11.2)',\n 'Requirement already satisfied: nltk in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (3.8.1)',\n 'Requirement already satisfied: sentencepiece in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.1.99)',\n 'Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sentence-transformers) (0.16.4)',\n 'Requirement already satisfied: filelock in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.3)',\n 'Requirement already satisfied: fsspec in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.0)',\n 'Requirement already satisfied: requests in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)',\n 'Requirement already satisfied: pyyaml>=5.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)',\n 'Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)',\n 'Requirement already satisfied: packaging>=20.9 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)',\n 'Requirement already satisfied: sympy in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)',\n 'Requirement already satisfied: networkx in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)',\n 'Requirement already satisfied: jinja2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)',\n 'Requirement already satisfied: colorama in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from tqdm->sentence-transformers) (0.4.6)',\n 'Requirement already satisfied: regex!=2019.12.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)',\n 'Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)',\n 'Requirement already satisfied: safetensors>=0.3.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)',\n 'Requirement already satisfied: click in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from nltk->sentence-transformers) (8.1.7)',\n 'Requirement already satisfied: joblib in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from nltk->sentence-transformers) (1.3.2)',\n 'Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)',\n 'Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from torchvision->sentence-transformers) (10.0.0)',\n 'Requirement already satisfied: MarkupSafe>=2.0 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)',\n 'Requirement already satisfied: charset-normalizer<4,>=2 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)',\n 'Requirement already satisfied: idna<4,>=2.5 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)',\n 'Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)',\n 'Requirement already satisfied: certifi>=2017.4.17 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)',\n 'Requirement already satisfied: mpmath>=0.19 in c:\\\\users\\\\qyzyr\\\\onedrive\\\\documents\\\\project_nlp\\\\unsupervised_annotator1\\\\venv\\\\lib\\\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)',\n '',\n '[notice] A new release of pip is available: 23.2.1 -> 23.3.1',\n '[notice] To update, run: python.exe -m pip install --upgrade pip']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!! pip install spacy\n",
    "!! python -m spacy download en_core_web_sm\n",
    "!!pip install sentence-transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:52.313035600Z",
     "start_time": "2023-10-30T08:59:14.430388Z"
    }
   },
   "id": "bb705793e4eb5a48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c44ac7a7abe1cd78"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens', device=device)\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.575440900Z",
     "start_time": "2023-10-30T08:59:51.942283800Z"
    }
   },
   "id": "7c57129dab585428"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "CandidateMWE = namedtuple('CandidateMWE',['text','head', 'sentence','self_encode', 'sent_encode'])\n",
    "CandidateW=namedtuple('CandidateW',['text','lemma', 'self_encode' ])\n",
    "# Term=namedtuple('Term',['text','detected' ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.575440900Z",
     "start_time": "2023-10-30T08:59:54.172892200Z"
    }
   },
   "id": "ca195029b0d8acd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "creating functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f1a8e78a4e42d9f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def parse_candidates(text:str):   \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract MWEs (noun phrases) from the text\n",
    "    mwe_list = []\n",
    "    single_noun_list=dict()\n",
    "    candidate_list=[]\n",
    "    for sent in doc.sents:\n",
    "        sent_encode=model.encode(sent.text, convert_to_tensor=True).to(device)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            is_candidate=False\n",
    "            word_count=0\n",
    "            # mwe_list.append(chunk.text)\n",
    "            if len(chunk.text.split()) > 1:\n",
    "                noun_appeared=False\n",
    "                is_candidate=True\n",
    "                cleared_candidate=''\n",
    "                for word in chunk:\n",
    "                    #IGNORING\n",
    "                    # if word.pos_ in ['PUNCT', 'DET']:\n",
    "                    #     continue\n",
    "                    # el\n",
    "                    if word.pos_ not in ['ADJ', 'PROPN', 'NOUN']:\n",
    "                        is_candidate=False\n",
    "                        # print(f'{chunk.text} is not candidate 1 {word.text} -- {word.pos_}')\n",
    "                        break\n",
    "                    elif word.pos_ in ['PROPN', 'NOUN']:\n",
    "                        noun_appeared=True\n",
    "                    elif not(not noun_appeared and word.pos_=='ADJ'):\n",
    "                        is_candidate=False\n",
    "                        # print(f'{chunk.text} is not candidate 2 {word.text} -- {word.pos_}, {noun_appeared}')\n",
    "                        break\n",
    "                    cleared_candidate+=' '+word.text\n",
    "                    word_count+=1\n",
    "            if is_candidate and word_count>1:\n",
    "                cleared_candidate=cleared_candidate.strip()\n",
    "                candidate_list.append(CandidateMWE(cleared_candidate, chunk.root.text, sent.text, model.encode(cleared_candidate, convert_to_tensor=True).to(device), sent_encode))\n",
    "                # print(f'Added candicate expression: {cleared_candidate}')\n",
    "            else:\n",
    "                \n",
    "                # print(f'Added candidate words from : {chunk.text}')    \n",
    "                for word in chunk:\n",
    "                    if word.pos_ in ['NOUN', 'PROPN']:\n",
    "                        single_noun_list[word.text]=  CandidateW(word.text, word.lemma_, model.encode(word.text))\n",
    "                        # print(word.text)    \n",
    "    return candidate_list, single_noun_list.values()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.575440900Z",
     "start_time": "2023-10-30T08:59:54.178144400Z"
    }
   },
   "id": "a9c58471f3da86bf"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def dist(wi_encode, wj_encode):\n",
    "    return util.pytorch_cos_sim(\n",
    "        wi_encode,\n",
    "        wj_encode\n",
    "    )\n",
    "def calculate_topic_score(expression_embedding, sentence_embedding)->float:\n",
    "    \"\"\"\n",
    "    Calculate the topic score between a multiword expression and a sentence.\n",
    "\n",
    "    Args:\n",
    "        multiword_expression (str): The multiword expression.\n",
    "        sentence (str): The sentence containing the expression.\n",
    "\n",
    "    Returns:\n",
    "        float: The topic score (cosine similarity) between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Load the distilbert-base-nli-mean-tokens model\n",
    "\n",
    "    # Encode the multiword expression and sentence into embeddings\n",
    "    # expression_embedding = model.encode(multiword_expression, convert_to_tensor=True)\n",
    "    # sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity between the two embeddings\n",
    "    similarity_score = util.pytorch_cos_sim(expression_embedding, sentence_embedding)\n",
    "\n",
    "    # Extract the cosine similarity value from the tensor\n",
    "    topic_score = similarity_score[0].item()\n",
    "\n",
    "    return topic_score\n",
    "\n",
    "\n",
    "def calculate_specificity_score(mw:CandidateMWE, full_encode:Tensor)->float:\n",
    "    \"\"\"\n",
    "    Calculate the specificity score (SP) between a multiword expression (mw) and a list of words/multiword expressions (w).\n",
    "\n",
    "    Args:\n",
    "        mw (str): The multiword expression.\n",
    "        w (list of str): The list of words/multiword expressions in the context.\n",
    "\n",
    "    Returns:\n",
    "        float: The specificity score (SP).\n",
    "    \"\"\"\n",
    "    # Load the distilbert-base-nli-mean-tokens model\n",
    "    # Calculate distances between mw and each word/phrase in w\n",
    "    distances = dist(mw.self_encode,full_encode)\n",
    "\n",
    "    # Calculate the mean of the distances\n",
    "    specificity_score = distances.mean().item()\n",
    "    \n",
    "\n",
    "    return specificity_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.183298700Z"
    }
   },
   "id": "550e916cd76d4fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def detect_mw_terms(candidate_list:List[CandidateMWE], TSP:float = 0.05, Ttopic:float = 0.1)->List[CandidateMWE]:    \n",
    "            \n",
    "    full_encode= torch.stack([wi.self_encode for wi in candidate_list], dim=0)    \n",
    "    \n",
    "    temp_candidate = []\n",
    "    for candidate in candidate_list:\n",
    "        topic_score = calculate_topic_score(candidate.self_encode, candidate.sent_encode)\n",
    "        sp_score=calculate_specificity_score(candidate, full_encode)\n",
    "        if topic_score > Ttopic and sp_score > TSP:\n",
    "            temp_candidate.append(candidate)\n",
    "            # print(f'Added \"{candidate.text}\" topic score {topic_score}, specifity score {sp_score}')\n",
    "\n",
    "    return temp_candidate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.189445300Z"
    }
   },
   "id": "2d085eeb13426a1d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def detect_single_noun_terms(term_mws:List[CandidateMWE], single_noun_list:List[CandidateW], subtoken_threshold:int=4)->List[CandidateW]:    \n",
    "    term_nouns=[]\n",
    "    for candidate in single_noun_list:\n",
    "        #Check if the lemma of the noun is the same as any of the heads of the multiword expressions.\n",
    "        is_term=False\n",
    "        lemma_is_head=False\n",
    "        for term_mw in term_mws:\n",
    "            if term_mw.head==candidate.lemma:\n",
    "                is_term=True\n",
    "                term_nouns.append(candidate)\n",
    "                # print(f'\"{candidate.text}\" is added by lemma: \"{candidate.lemma}\" is head of \"{term_mw}\"')\n",
    "                break\n",
    "        if is_term:\n",
    "            continue\n",
    "        #segment the word using a subword-unit segmentation and a vocabulary trained over a large general purpose corpus.\n",
    "        subtokens=tokenizer.tokenize(candidate.text)\n",
    "        if len(subtokens)>subtoken_threshold:\n",
    "            term_nouns.append(candidate)\n",
    "            # print(f'{candidate.text} is added by subtokens count: {len(subtokens)}')\n",
    "    return term_nouns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.193143500Z"
    }
   },
   "id": "682a0aaba8edf6ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate metrics(from https://colab.research.google.com/drive/1y9WM3MSAEwvODhMt0cMwJi24XIllrmaY?usp=sharing) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe36b79e31abe8f1"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def calculate_metrics(true_terms, extracted_terms):\n",
    "    true_positives = len(true_terms.intersection(extracted_terms))\n",
    "    false_positives = len(extracted_terms.difference(true_terms))\n",
    "    false_negatives = len(true_terms.difference(extracted_terms))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def compare_sets(extracted_terms, true_terms):\n",
    "    true_detections = extracted_terms.intersection(true_terms)\n",
    "    false_gaps = true_terms.difference(extracted_terms)\n",
    "    false_detections = extracted_terms.difference(true_terms)\n",
    "\n",
    "    return true_detections, false_gaps, false_detections"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.197189100Z"
    }
   },
   "id": "9e2889ce9dd8b338"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def report(true_terms, extracted_terms):\n",
    "    # Расчет метрик\n",
    "    precision, recall, f1_score = calculate_metrics(true_terms, extracted_terms)\n",
    "    \n",
    "    print(\"Precision:\", precision) #Точность\n",
    "    print(\"Recall:\", recall) #Полнота\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    # Выводит списки истинных обнаружений, ложных пропуски и ложных обнаружений\n",
    "    true_detections, false_gaps, false_detections = compare_sets(extracted_terms, true_terms)\n",
    "    \n",
    "    print(\"Истинные обнаружения:\", true_detections)\n",
    "    print(\"Ложные пропуски:\", false_gaps)\n",
    "    print(\"Ложные обнаружения:\", false_detections)\n",
    "    return precision, recall, f1_score, true_detections, false_gaps, false_detections "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.204052200Z"
    }
   },
   "id": "5171f8f0a4386690"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.206724800Z"
    }
   },
   "id": "cb2a35c8f824c8c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing in Folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec79888dda3aa91"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_name='tests'\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(folder_name)\n",
    "\n",
    "# Initialize empty lists to store the tuples\n",
    "file_tuples = []\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for filename in all_files:\n",
    "    # Check if the file is a test file (starts with \"text_\")\n",
    "    if filename.startswith(\"text_\"):\n",
    "        # Construct the expected result filename by replacing \"text_\" with \"term_\"\n",
    "        result_filename = \"term_\" + filename[5:]\n",
    "        \n",
    "        # Check if the corresponding result file exists in the folder\n",
    "        if result_filename in all_files:\n",
    "            # Append the tuple to the list\n",
    "            file_tuples.append((filename, result_filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.210399600Z"
    }
   },
   "id": "97d37b7434e321f3"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[('text_1.txt', 'term_1.txt'), ('text_2.txt', 'term_2.txt')]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tuples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.215694300Z"
    }
   },
   "id": "6533871b027572d7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def test_file(text_file_name, term_file_name):\n",
    "    print('____________________________________________________________________\\n')\n",
    "    print(f\"checking files: {text_file_name}, {term_file_name}\")    \n",
    "    text=''\n",
    "    with open(text_file_name) as text_file:\n",
    "        text=text_file.read()\n",
    "    print('_____________________________FIRST STEP_________________________________________')\n",
    "    candidate_list, single_noun_list=parse_candidates(text)\n",
    "    print('_____________________________SECOND STEP___________________________________________________')\n",
    "    term_mws=detect_mw_terms(candidate_list)\n",
    "    print('_____________________________THIRD STEP______________________________________________________________')\n",
    "    term_nouns=detect_single_noun_terms(term_mws, single_noun_list)\n",
    "    print('____________________________________TESTING__________________________________________________________________')\n",
    "    extracted_terms=set([txt.text.lower() for txt in term_mws+term_nouns])\n",
    "    with open(term_file_name) as term_file:\n",
    "        terms_str=term_file.read()\n",
    "    true_terms=set(terms_str.lower().split(', '))\n",
    "\n",
    "    precision, recall, f1_score, true_detections, false_gaps, false_detections = report(true_terms, extracted_terms)\n",
    "    return true_terms, extracted_terms, precision, recall, f1_score, true_detections, false_gaps, false_detections \n",
    " \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T08:59:54.576441100Z",
     "start_time": "2023-10-30T08:59:54.218917400Z"
    }
   },
   "id": "fae850ba26da2b19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________\n",
      "\n",
      "checking files: text_1.txt, term_1.txt\n",
      "_____________________________FIRST STEP_________________________________________\n"
     ]
    }
   ],
   "source": [
    "os.chdir(folder_name)\n",
    "whole_true_terms=[]\n",
    "whole_extracted_terms=[]\n",
    "results=[]\n",
    "for text_file_name, term_file_name in file_tuples:\n",
    "    true_terms, extracted_terms, precision, recall, f1_score, true_detections, false_gaps, false_detections=test_file(text_file_name, term_file_name)\n",
    "    results.append((true_terms, extracted_terms, precision, recall, f1_score, true_detections, false_gaps, false_detections ))\n",
    "    print('____________________________________________________________________\\n')\n",
    "    whole_true_terms+=true_terms\n",
    "    whole_extracted_terms=extracted_terms\n",
    "print('___________________________TOTAL_________________________________________\\n')\n",
    "whole_true_terms=set(whole_true_terms)\n",
    "whole_extracted_terms=set(whole_extracted_terms)\n",
    "print('total result')\n",
    "report(whole_true_terms, whole_extracted_terms)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-30T08:59:54.224285200Z"
    }
   },
   "id": "f07c640dc1e74e39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir('..')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2bb33be2aa83c996"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fa44f4fdf3a91839"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
